<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Christina Shin</title>
  <meta name="description" content="Personal website of Christina">
  <link rel="stylesheet" href="https://usc-nsl.github.io/css/main.css">
  <link rel="canonical" href="https://usc-nsl.github.io/people/christina-shin/">
  <!-- <link rel="shortcut icon" type ="image/x-icon" href="https://usc-nsl.github.io/images/usc_favicon.png"> -->
  <link rel="shortcut icon" href="https://www.usc.edu/wp-content/themes/usc-homepage-2017/assets/images/favicon.ico?v=4.3.14" />
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css"> -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
	  <a class="navbar-brand" href="https://usc-nsl.github.io/">
		<div class="logo-image">
			  <img src="/images/logopic/nsl_logo_3_Raj.png" height="44px" class="img-fluid">
		</div>
	  </a>  
    <a class="navbar-brand" href="https://usc-nsl.github.io/">NSL @ USC</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="https://usc-nsl.github.io/">Home</a></li>
		<li><a href="https://usc-nsl.github.io/people">Team</a></li>
		<!-- <li><a href="https://usc-nsl.github.io/vacancies">Vacancies</a></li> -->
		<li><a href="https://usc-nsl.github.io/publications">Publications</a></li>
		<!-- <li><a href="https://usc-nsl.github.io/research">Research</a></li> -->
		<!-- <li><a href="https://usc-nsl.github.io/pictures">(Pics)</a></li> -->
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">


<div id="gridid" class="col-sm-12">
	<div class="row">
  <p><img src="https://usc-nsl.github.io/images/teampic/Christina/christina.jpg" class="img-responsive" width="22%" style="float: left"></p>
  <h1>Christina Shin</h1>
  <p><i style="font-size:20px">Ph.D. Student, <a href="https://nsl.usc.edu/" target="_blank" rel="noopener noreferrer">USC@NSL</a></i><br></p>

  <p><a href="mailto:cshin956@usc.edu" target="_blank"><i class="fa fa-envelope-square fa-3x"></i></a> 
   <a href="https://scholar.google.com/citations?user=6VwIYFwAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar-square ai-3x"></i></a> 
   <a href="https://usc-nsl.github.io/files/2024-10-01-Christina-CV.pdf" target="_blank"><i class="ai ai-cv-square ai-3x"></i></a> 
   <a href="https://github.com/christina-shin" target="_blank" rel="noopener noreferrer"><i class="fa fa-github-square fa-3x"></i></a> 
   <a href="https://www.linkedin.com/in/christina-shin-9477a31a1" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin-square fa-3x"></i></a> 
   <a href="https://twitter.com/christina3_3" target="_blank" rel="noopener noreferrer"><i class="fa fa-twitter-square fa-3x"></i></a> 
  <!--  --></p>
  <ul style="overflow: hidden">

  
	<li> (2019-Present) Ph.D. in Computer Science, University of Southern California, Los Angeles, USA. </li>
  
	<li> (2017-2019) M.S. in Computer Science and Engineering, Ewha Womans University, Seoul, South Korea. </li>
  
	<li> (2012-2017) B.S. in Computer Science and Engineering, Ewha Womans University, Seoul, South Korea. </li>
  

  </ul>
</div>

<h2 id="sketch">Sketch</h2>

<p>
I am a fifth year Ph.D. student in <a href="https://www.cs.usc.edu/" target="_blank" rel="noopener noreferrer">Computer Science Department</a> at <a href="http://www.usc.edu" target="_blank" rel="noopener noreferrer">University of Southern California</a>. I am working with <a href="https://govindan.usc.edu/" target="_blank" rel="noopener noreferrer">Prof. Ramesh Govindan</a> in <a href="https://nsl.usc.edu/" target="_blank" rel="noopener noreferrer">Networked Systems Lab (NSL)</a>. I am broadly interested in Volumetric Video Streaming, 3D Sensing, Cooperative Perception, and Autonomous Vehicle Systems. Before joining NSL, I received my B.S. and M.S. degree in Computer Science and Engineering at <a href="https://www.ewha.ac.kr/ewhaen/index.do" target="_blank" rel="noopener noreferrer">Ewha Womans University</a>.
</p>

<h2 id="work-experience">Work Experience</h2>

<p>
<em><strong>Research Assistant (Aug 2019 - Present)</strong></em><br>
<a href="https://nsl.usc.edu/" target="_blank" rel="noopener noreferrer">Networked Systems Lab</a>, University of Southern California, Los Angeles, USA.<br>
</p>

<p>
<em><strong>Research Intern (May 2021 - Aug 2021, May 2024 - Aug 2024)</strong></em><br>
General Motors Research and Development, Warren, USA.<br>
Mentor: Chuan Li and Fan Bai<br>
</p>

<p>
<em><strong>Research Assistant (Jan 2017 - May 2019)</strong></em><br>
<a href="https://inslab-ewha.weebly.com/" target="_blank" rel="noopener noreferrer">Intelligent Networked Systems Lab</a>, Ewha Womans University, Seoul, South Korea.<br>
</p>

<h2 id="publications">Publications</h2>

<div class="publications">

  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">MobiCom</abbr></div>

        <!-- Entry bib key -->
        <div id="shin2024recap" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">RECAP: 3D Traffic Reconstruction</div>
          <!-- Author -->
          <div class="author">Shin, Christina, Pang, Weiwu, Li, Chuan, Bai, Fan, Ahmad, Fawad, Paek, Jeongyeup, and Govindan, Ramesh
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking (MobiCom)</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="" class="btn btn-sm z-depth-0" role="button">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>On-vehicle 3D sensing technologies, such as LiDARs and stereo cameras, enable a novel capability, 3D traffic reconstruction. This produces a volumetric video consisting of a sequence of 3D frames capturing the time evolution of road traffic. 3D traffic reconstruction can help trained investigators reconstruct the scene of an accident. In this paper, we describe the design and implementation of RECAP, a system that continuously and opportunistically produces 3D traffic reconstructions from multiple vehicles. RECAP builds upon prior work on point cloud registration, but adapts it to settings with minimal point cloud overlap (both in the spatial and temporal sense) and develops techniques to minimize error and computation time in multi-way registration. On-road experiments and trace-driven simulations show that RECAP can, within minutes, generate highly accurate reconstructions that have 2× or more lower errors than competing approaches.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IoTDI</abbr></div>

        <!-- Entry bib key -->
        <div id="ahmad2024cip" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Cooperative Infrastructure Perception</div>
          <!-- Author -->
          <div class="author">Ahmad, Fawad, Shin, Christina, Pang, Weiwu, Leong, Branden, Ghosh, Pradipta, and Govindan, Ramesh
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ACM/IEEE Conference on Internet of Things Design and Implementation (IoTDI)</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.ieeecomputersociety.org/10.1109/IoTDI61053.2024.00010" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>Recent works have considered two qualitatively different approaches to overcome line-of-sight limitations of 3D sensors used for perception: cooperative perception and infrastructure-augmented perception. In this paper, motivated by increasing deployments of infrastructure LiDARs, we explore a third approach – cooperative infrastructure perception. This approach generates perception outputs by fusing outputs of multiple infrastructure sensors, but, to be useful, must do so quickly and accurately. We describe the design, implementation and evaluation of Cooperative Infrastructure Perception (CIP), which uses a combination of novel algorithms and systems optimizations. It produces perception outputs within 100 ms using modest computing resources and with accuracy comparable to the state-of-the-art. CIP, when used to augment vehicle perception, can improve safety. When used in conjunction with offloaded planning, CIP can increase traffic throughput at intersections.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Ubicomp/IMWUT</abbr></div>

        <!-- Entry bib key -->
        <div id="ahmad2023aerotraj" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">AeroTraj: Trajectory Planning for Fast, and Accurate 3D Reconstruction Using a Drone-based LiDAR</div>
          <!-- Author -->
          <div class="author">Ahmad, Fawad, Shin, Christina, Ghosh, Rajrup, D’Ambrosio, John, Chai, Eugene, Sundaresan, Karthikeyan, and Govindan, Ramesh
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (Ubicomp/IMWUT)</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dl.acm.org/doi/abs/10.1145/3610911" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>This paper presents AeroTraj, a system that enables fast, accurate, and automated reconstruction of 3D models of large buildings using a drone-mounted LiDAR. LiDAR point clouds can be used directly to assemble 3D models if their positions are accurately determined. AeroTraj uses SLAM for this, but must ensure complete and accurate reconstruction while minimizing drone battery usage. Doing this requires balancing competing constraints: drone speed, height, and orientation. AeroTraj exploits building geometry in designing an optimal trajectory that incorporates these constraints. Even with an optimal trajectory, SLAM’s position error can drift over time, so AeroTraj tracks drift in-flight by offloading computations to the cloud and invokes a re-calibration procedure to minimize error. AeroTraj can reconstruct large structures with centimeter-level accuracy and with an average end-to-end latency below 250 ms, significantly outperforming the state of the art.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TVT</abbr></div>

        <!-- Entry bib key -->
        <div id="shin2020infrastructure" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Infrastructure-less vehicle traffic density estimation via distributed packet probing in v2v network</div>
          <!-- Author -->
          <div class="author">Shin, Christina, Lee, JiHo, and Lee, HyungJune
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE Transactions on Vehicular Technology (TVT)</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9178450" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>In this paper, we address the problem of vehicle traffic density estimation without relying on infrastructure cameras or sensors on the road. Previous infrastructure-less approaches still require some prior knowledge on the road infrastructure, e.g., via road topology map. We seek a lightweight estimation method based only on vehicle-to-vehicle (V2V) communication, i.e., without using any prior knowledge. The main objective of this paper is to examine traffic density through simple yet efficient packet probing within a survey time period and obtain a snapshot of the traffic density distribution map. We propose an on-demand vehicle sampling algorithm that makes a probing packet at a vehicle (i.e., sampler) keep sampling to explore the local traffic density on a cell basis. If a current sampler does not operate as an efficient carrier, the packet selects another one as the next sampler via inner-relaying and outer-relaying procedures. To effectively adapt the level of granularity of traffic density depending on the remaining survey time, we present an adaptive cell sizing algorithm. Further, we extend the sampling activity to multiple vehicle samplers by making them aggregate their collected information and also negotiate their future areas to explore. Within a designated deadline, multiple samplers collaborate for more accurate and fast traffic density estimation. By doing so by iterations till the given survey deadline, we can gather a complete view of traffic density estimates based on multiple sources where some areas have more detailed information, whereas others do less. Experiments with a real trace-driven simulation demonstrate that our proposed algorithm effectively estimates the distribution of traffic density considering local traffic conditions compared to other counterpart algorithms, with a factor of up to 9.5.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TVT</abbr></div>

        <!-- Entry bib key -->
        <div id="park2018dronenetx" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DroneNetX: Network reconstruction through connectivity probing and relay deployment by multiple UAVs in ad hoc networks</div>
          <!-- Author -->
          <div class="author">Park, So-Yeon, Shin, Christina, Jeong, Dahee, and Lee, HyungJune
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE Transactions on Vehicular Technology (TVT)</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8466046" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>In this paper, we consider a network reconstruction problem using unmanned aerial vehicles (UAVs) where stationary ad hoc networks are severely damaged in a post-disaster scenario. The main objective of this paper is to repair the network by supplementing aerial wireless links into the isolated ground network using UAVs. Our scheme performs network probing from the air and finds out crucial spots where both local and global routing performance can significantly be recovered if deployed. First, we propose a novel distributed coverage path planning algorithms with independent and computationally lightweight navigation based on adaptive zigzag patterns. Second, we present route topology discovery schemes that capture both local and non -local network connectivity by extracting inherent route skeletons via stitching partial local paths obtained from the simple packet probing by UAVs. Finally, we find the optimal UAV relay deployment positions that can improve network-wide data delivery most effectively based on three novel approaches of an optimization technique, an iterative heuristic algorithm, and a topology partitioning of strongly connected component . Simulation results demonstrate that our distributed traversing algorithms reduce the complete coverage time, the travel distance, and the duplicate coverage compared to other counterpart algorithms. Our deployment algorithms recover severely impaired routes, incurring reasonable computational overhead.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">WCNC</abbr></div>

        <!-- Entry bib key -->
        <div id="shin2018progressive" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Progressive ad-hoc route reconstruction using distributed UAV relays after a large-scale failure</div>
          <!-- Author -->
          <div class="author">Shin, Christina, Park, So-Yeon, Yoon, JinYi, and Lee, HyungJune
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE Wireless Communications and Networking Conference (WCNC)</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8377012" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>In this paper, we address a route reconstruction problem using Unmanned Aerial Vehicles (UAVs) after a large-scale disaster where stationary ad-hoc networks are severely destructed. The main goal of this paper is to improve routing performance in a progressive manner by reconnecting partitioned networks through dispatched UAV relays. Our proposed algorithm uses two types of UAVs: global and local UAVs to collaboratively find the best deployment position in a dynamically changing environment. To obtain terrestrial network connectivity information and extract high-level network topology, we exploit the concept of strongly connected component in graph theory. Based on the understanding from a global point view, global UAVs recommend the most effective deployment positions to local UAVs so that they are deployed as relays in more critically disrupted areas. Simulation-based experiments validate that our distributed route reconstruction algorithm outperforms a counterpart algorithm in terms of steady-state and dynamic routing performance.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">GLOBECOM</abbr></div>

        <!-- Entry bib key -->
        <div id="park2017dronenet+" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DroneNet+: Adaptive route recovery using path stitching of UAVs in ad-hoc networks</div>
          <!-- Author -->
          <div class="author">Park, So-Yeon, Jeong, Dahee, Shin, Christina, and Lee, HyungJune
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE Global Communications Conference (GLOBECOM)</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8253970" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract bibhidden">
            <p>In this paper, we consider a route recovery problem using Unmanned Aerial Vehicles (UAVs) as relay nodes to connect with terrestrial ad-hoc networks in realistic disaster scenarios. Our main goal is to perform network probing from the air by UAVs and find out crucial spots where both local and global routing performance can significantly be recovered if they are deployed. We propose a route topology discovery scheme that extracts the inherent route skeletons by stitching partial local paths obtained from simple packet probing by UAVs, while exploring a designated Region of Interest (RoI) by an adaptive traversing scheme. By leveraging the captured topology, we dispatch a limited number of UAVs by an iterative UAV deployment algorithm and provide a lightweight yet effective network hole replacement decision in a heuristic manner. Simulation results demonstrate that our traversing algorithm reduces the complete coverage time, the travel distance, and the duplicate coverage compared to a previous work, DroneNet. Our subsequent iterative deployment algorithm greatly recovers severely impaired routes in a damaged network, while substantially reducing computational complexity.</p>
          </div>
        </div>
      </div>
</li>
</ol>

</div>

</div>
      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5 footer">
			
		  <p>© NSL Lab. Site made with <a href="https://jekyllrb.com" target="_blank" rel="noopener noreferrer">Jekyll</a></p>
              <p>We are part of the <a href="https://www.cs.usc.edu/" target="_blank" rel="noopener noreferrer">USC Viterbi Department of Computer Science</a> at the <a href="https://www.usc.edu/" target="_blank" rel="noopener noreferrer">University of Southern California</a>.</p>
		</div>
		<!-- <div class="col-sm-4">
		  Funding:<br />
		  - <a href="http://www.nwo.nl/en/research-and-results/programmes/Talent+Scheme">Vidi </a> and <a href='https://www.fom.nl/en/news/press-releases/2016/11/18/28634/'>Projectruimte</a> grants from <a href="http://www.nwo.nl">NWO</a> <br />
		  - <a href="https://www.universiteitleiden.nl/en/research/research-projects/science/frontiers-of-nanoscience-nanofront">Frontier of Nanosciences</a>, a gravity program from <a href="http://www.nwo.nl">NWO</a>
          - <a href='https://www.universiteitleiden.nl/en/news/2017/08/two-erc-grants-for-leiden-physics'>ERC starting grant</a>
		</div> -->
		
		<div class="col-sm-5 footer">
		  <b>Location:</b><br>
		  Salvatori Computer Science Center (SAL), USC<br>
		  941 Bloom Walk<br>
		  Los Angeles, CA 90089 (<a href="https://goo.gl/maps/KiKEz2XTyFFifWPw6" target="_blank" rel="noopener noreferrer">Maps</a>)
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://usc-nsl.github.io/js/bootstrap.min.js"></script>


    <!-- Load Common JS -->
  <script src="/js/common.js"></script>


  </body>

</html>
